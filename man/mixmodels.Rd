\name{mixmodels}
\alias{mixmodels}
\alias{mixModelsTrain}
\alias{mixModelsTrain,matrix-method}
\alias{mixModelsTrain,ExpressionSet-method}
\alias{mixModelsTest}
\alias{mixModelsTest,list,matrix-method}
\alias{mixModelsTest,list,ExpressionSet-method}
\title{Selection of Differential Distributions with Mixtures of Normals}
\description{Fits mixtures of normals for every gene, separately for each class.
       }
\usage{
  \S4method{mixModelsTrain}{matrix}(expression, classes, ...)
  \S4method{mixModelsTrain}{ExpressionSet}(expression, ..., verbose = 3)
  \S4method{mixModelsTest}{list,matrix}(models, test, ...)
  \S4method{mixModelsTest}{list,ExpressionSet}(models, test,
            weighted = c("both", "unweighted", "weighted"),
            weight = c("all", "height difference", "crossover distance", "sum differences"),
            densityXvalues = 1024, minDifference = 0, tolerance = 0.01,
            returnType = c("label", "score", "both"), verbose = 3)  
}
\arguments{
  \item{expression}{Either a \code{\link{matrix}} or \code{\link{ExpressionSet}} containing
                    the training data. For a matrix, the rows are features, and the columns
                    are samples.}
  \item{test}{Either a \code{\link{matrix}} or \code{\link{ExpressionSet}} containing
                    the test data. For a matrix, the rows are features, and the columns
                    are samples.}                    
  \item{classes}{A vector of class labels.}
  \item{weighted}{In weighted mode, the difference in densities is summed over all features.
                  If unweighted mode, each features's vote is worth the same. To save
                  computational time, both can be calculated simultaneously.}
  \item{weight}{The type of weight to calculate. For \code{"height difference"}, the weight of
                each prediction is equal to the sum of the verical distances for all of the
                mixture components within one class subtracted from the sum of the components of
                the other class, summed for each value of x. For \code{"crossover distance"},
                the x positions where two mixture densities cross is firstly calculated.
                The predicted class is the class with the highest mixture sum at the
                particular value of x and the weight is the distance of x from the
                nearest density crossover point.}
  \item{densityXvalues}{Only relevant when \code{weight} is \code{"crossover distance"}. The number of
                        equally-spaced locations at which to calculate y values for each mixture density.}
  \item{minDifference}{The minimum difference in sums of mixture densities within each class for a
                       feature to be allowed to vote. Can be a vector of cutoffs. If no features for a
                       particular sample have a difference large enough, the class predicted is
                       simply the largest class.}
  \item{tolerance}{Only relevant when \code{weight} is \code{"crossover distance"}. Absolute differences in
                   the sums of y values of two densities of this magnitude or smaller cause
                   the densities at the corresponding x values to be considered as overlapping.}            
  \item{...}{For the training or testing function with \code{\link{matrix}} dispatch,
             arguments passed to the function with \code{\link{ExpressionSet}} dispatch.
             For the training function with \code{\link{ExpressionSet}} dispatch,
             extra arguments passed to \code{\link[Rmixmod]{mixmodCluster}}. The argument \code{nbCluster}
             is mandatory.}                       
  \item{models}{A list of length 2 of models generated by the training function.
                The first element has mixture models the same length as the number
                of features in the expression data for one class. The second element
                has the same information for the other class.}
  \item{returnType}{Either \code{"label"}, \code{"score"}, or \code{"both"}. Sets the return value
                    from the prediction to either a vector of class labels, score for a sample belonging
                    to the second class, as determined by the factor levels, or both labels and scores
                    in a \code{\link{data.frame}}.}                     
  \item{verbose}{A number between 0 and 3 for the amount of progress messages to give.
                 A higher number will produce more messages.}
}
\details{
  If \code{weighted} is \code{TRUE}, then a sample's predicted class is the class with
  the largest sum of weights, scaled for the number of samples in
  the training data of each class. Otherwise, when \code{weighted} is \code{FALSE},
  each feature has an equal vote, and votes for the class with the largest weight, scaled for
  class sizes in the training set.

  If \code{weight} is \code{"crossover distance"}, the crossover points are computed by considering the 
  distance between y values of the two mixture densities at every x value. If the y values are sufficiently close,
  the corresponding x values added to a candidate list. Consecutive x values are grouped, and the x value
  in each group that has with the smallest distance is chosen as the representative location of
  the crossover point. Only y values that are \code{tolerance} or greater are considered in this first stage.
  If no crossover points are found, the y values below \code{tolerance} are considered, except for those at the
  leftmost or rightmost region of the range of the densities. This is necessary when the densities
  are completely separated.
  
  Setting weight to \code{"sum differences"} is intended to find a mix of features which are strongly
  differentially expressed and differentially variable.  
}
\value{
  For \code{mixModelsTrain}, a list of trained models of class \code{\link[Rmixmod:MixmodCluster-class]{MixmodCluster}}.
  A vector or list of class prediction information, as long as the number of samples in the test data,
  or lists of such information, if both weighted and unweighted voting or a range of \code{minDifference}
  values was provided.
}
\author{Dario Strbenac}

\examples{
  # First 25 samples are mixtures of two normals. Last 25 samples are one normal.
  genesMatrix <- sapply(1:25, function(geneColumn) c(rnorm(50, 5, 1), rnorm(50, 15, 1)))
  genesMatrix <- cbind(genesMatrix, sapply(1:25, function(geneColumn) rnorm(100, 9, 3)))
  classes <- factor(rep(c("Poor", "Good"), each = 25))
  trained <- mixModelsTrain(genesMatrix, classes, nbCluster = 1:3)
  mixModelsTest(trained, genesMatrix, minDifference = 1:3)
}
