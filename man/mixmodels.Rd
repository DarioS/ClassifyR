\name{mixmodels}
\alias{mixmodels}
\alias{mixModelsTrain}
\alias{mixModelsTrain,matrix-method}
\alias{mixModelsTrain,DataFrame-method}
\alias{mixModelsTrain,MultiAssayExperiment-method}
\alias{mixModelsPredict}
\alias{mixModelsPredict,list,matrix-method}
\alias{mixModelsPredict,list,DataFrame-method}
\alias{mixModelsPredict,list,MultiAssayExperiment-method}
\title{Classification based on Differential Distribution utilising Mixtures of Normals}
\description{Fits mixtures of normals for every feature, separately for each class.
}
\usage{
  \S4method{mixModelsTrain}{matrix}(measurements, ...)
  \S4method{mixModelsTrain}{DataFrame}(measurements, classes, ..., verbose = 3)
  \S4method{mixModelsTrain}{MultiAssayExperiment}(measurements, targets = names(measurements), ...)
  \S4method{mixModelsPredict}{list,matrix}(models, test, ...)
  \S4method{mixModelsPredict}{list,DataFrame}(models, test, weighted = c("unweighted", "weighted", "both"),
                  weight = c("height difference", "crossover distance", "both"),
              densityXvalues = 1024, minDifference = 0,
              returnType = c("class", "score", "both"), verbose = 3)
  \S4method{mixModelsPredict}{list,MultiAssayExperiment}(models, test, targets = names(test), ...)  
}
\arguments{
  \item{measurements}{Either a \code{\link{matrix}}, \code{\link{DataFrame}} or
                       \code{\link{MultiAssayExperiment}} containing the training data.
                       For a \code{matrix}, the rows are features, and the columns are samples.}
  \item{classes}{Either a vector of class labels of class \code{\link{factor}} of the same length
                 as the number of samples in \code{measurements} or if the measurements are
                 of class \code{DataFrame} a character vector of length 1 containing the
                 column name in \code{measurement} is also permitted. Not used if \code{measurements}
                 is a \code{MultiAssayExperiment} object.}
  \item{test}{An object of the same class as \code{measurements} with no samples in common with
              \code{measurements} and the same number of features as it. Also, if a \code{DataFrame},
              the \code{class} column must be absent.}
  \item{targets}{If \code{measurements} is a \code{MultiAssayExperiment}, the names of the
                 data tables to be used. \code{"clinical"} is also a valid value and specifies that
                 numeric variables from the clinical data table will be used.}
  \item{...}{Variables not used by the \code{matrix} nor the \code{MultiAssayExperiment} method which
             are passed into and used by the \code{DataFrame} method or extra arguments for training passed to
             \code{\link[Rmixmod]{mixmodCluster}}. The argument \code{nbCluster} is mandatory.}
  \item{models}{A list of length 3 of models generated by the training function and training class
                information. The first element has mixture models the same length as the number
                of features in the expression data for one class. The second element
                has the same information for the other class. The third element has the class sizes of
                the classes in the training data.}             
  \item{weighted}{Default: \code{"unweighted"}. Either \code{"unweighted"}, \code{"weighted"} or \code{"both"}. 
                  In weighted mode, the difference in densities is summed over all features.
                  If unweighted mode, each feature's vote is worth the same. Both can be
                  calculated simultaneously.}
  \item{weight}{Default: \code{"both"}. Either \code{"both"}, \code{"height difference"}, or
                \code{"crossover distance"}. The type of weight to calculate. For \code{"height difference"},
                the weight of each prediction is equal to the sum of the vertical distances for all of the
                mixture components within one class subtracted from the sum of the components of
                the other class, summed for each value of x. For \code{"crossover distance"},
                the x positions where two mixture densities cross is firstly calculated.
                The predicted class is the class with the highest mixture sum at the
                particular value of x and the weight is the distance of x from the
                nearest density crossover point.}
  \item{densityXvalues}{Default: 1024. Only relevant when \code{weight} is \code{"crossover distance"}. The number of
                        equally-spaced locations at which to calculate y values for each mixture density.}
  \item{minDifference}{Default: 0. The minimum difference in sums of mixture densities within each class for a
                       feature to be allowed to vote. Can be a vector of cutoffs. If no features for a
                       particular sample have a difference large enough, the class predicted is
                       simply the largest class.}
  \item{returnType}{Default: \code{"class"}. Either \code{"class"}, \code{"score"} or \code{"both"}. Sets the return value
                    from the prediction to either a vector of class labels, score for a sample belonging
                    to the second class, as determined by the factor levels, or both labels and scores
                    in a \code{data.frame}.}                     
  \item{verbose}{Default: 3. A number between 0 and 3 for the amount of progress messages to give.
                 This function only prints progress messages if the value is 3.}
}
\details{
  If \code{weighted} is \code{TRUE}, then a sample's predicted class is the class with
  the largest sum of weights, each scaled for the number of samples in
  the training data of each class. Otherwise, when \code{weighted} is \code{FALSE},
  each feature has an equal vote, and votes for the class with the largest weight, scaled for
  class sizes in the training set.

  If \code{weight} is \code{"crossover distance"}, the crossover points are computed by considering the 
  distance between y values of the two densities at every x value. x values for which the sign of the
  difference changes compared to the difference of the closest lower value of x are used as
  the crossover points.
}
\value{
  For \code{mixModelsTrain}, a list of trained models of class \code{\link[Rmixmod:MixmodCluster-class]{MixmodCluster}}.
  For \code{mixModelsPredict}, a vector or list of class prediction information, as long as the
  number of samples in the test data, or lists of such information, if both weighted and unweighted voting
  or a range of \code{minDifference} values was provided.
}
\author{Dario Strbenac}

\examples{
  # First 25 samples and first 5 genes are mixtures of two normals. Last 25 samples are
  # one normal.
  
  genesMatrix <- sapply(1:25, function(geneColumn) c(rnorm(5, sample(c(5, 15), replace = TRUE, 5))))
  genesMatrix <- cbind(genesMatrix, sapply(1:25, function(geneColumn) c(rnorm(5, 9, 1))))
  genesMatrix <- rbind(genesMatrix, sapply(1:50, function(geneColumn) rnorm(5, 9, 1)))
  rownames(genesMatrix) <- paste("Gene", 1:10)
  colnames(genesMatrix) <- paste("Sample", 1:50)
  classes <- factor(rep(c("Poor", "Good"), each = 25), levels = c("Good", "Poor"))
  
  trainSamples <- c(1:15, 26:40)
  testSamples <- c(16:25, 41:50)
  
  trained <- mixModelsTrain(genesMatrix[, trainSamples], classes[trainSamples],
                            nbCluster = 1:3)
  mixModelsPredict(trained, genesMatrix[, testSamples], minDifference = 0:3)
}
