% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/interfaceNaiveBayesKernel.R
\name{naiveBayesKernel}
\alias{naiveBayesKernel}
\alias{naiveBayesKernel,matrix-method}
\alias{naiveBayesKernel,DataFrame-method}
\alias{naiveBayesKernel,MultiAssayExperiment-method}
\title{Classification Using A Bayes Classifier with Kernel Density Estimates}
\usage{
\S4method{naiveBayesKernel}{matrix}(measurementsTrain, classesTrain, measurementsTest, ...)

\S4method{naiveBayesKernel}{DataFrame}(
  measurementsTrain,
  classesTrain,
  measurementsTest,
  densityFunction = density,
  densityParameters = list(bw = "nrd0", n = 1024, from =
    expression(min(featureValues)), to = expression(max(featureValues))),
  difference = c("unweighted", "weighted"),
  weighting = c("height difference", "crossover distance"),
  minDifference = 0,
  returnType = c("both", "class", "score"),
  verbose = 3
)

\S4method{naiveBayesKernel}{MultiAssayExperiment}(
  measurementsTrain,
  measurementsTest,
  targets = names(measurements),
  classesTrain,
  ...
)
}
\arguments{
\item{measurementsTrain}{Either a \code{\link{matrix}}, \code{\link{DataFrame}}
or \code{\link{MultiAssayExperiment}} containing the training data. For a
\code{matrix} or \code{\link{DataFrame}}, the rows are samples, and the columns are features.
If of type \code{\link{DataFrame}} or \code{\link{MultiAssayExperiment}}, the data set is subset
to only those features of type \code{numeric}.}

\item{...}{Unused variables by the three top-level methods passed to the
internal method which does the classification.}

\item{classesTrain}{A vector of class labels of class \code{\link{factor}} of the
same length as the number of samples in \code{measurementsTrain} if it is a
\code{\link{matrix}} or a \code{\link{DataFrame}} or a character vector of length 1
containing the column name in \code{measurementsTrain} if it is a \code{\link{DataFrame}} or the
column name in \code{colData(measurementsTrain)} if \code{measurementsTrain} is a
\code{\link{MultiAssayExperiment}}. If a column name, that column will be
removed before training.}

\item{measurementsTest}{An object of the same class as \code{measurementsTrain} with no
samples in common with \code{measurementsTrain} and the same number of features
as it.}

\item{densityFunction}{Default: \code{\link{density}}. A function which will
return a probability density, which is essentially a list with x and y
coordinates.}

\item{densityParameters}{A list of options for \code{densityFunction}.
Default: \code{list(bw = "nrd0", n = 1024, from =
expression(min(featureValues)), to = expression(max(featureValues))}.}

\item{difference}{Default: \code{"unweighted"}. Either \code{"unweighted"},
\code{"weighted"}. In weighted mode, the difference in densities is summed
over all features. If unweighted mode, each feature's vote is worth the
same.}

\item{weighting}{Default: \code{"height difference"}. Either \code{"height difference"}
or \code{"height difference"}. The type of weight to calculate. For
\code{"height difference"}, the weight of each prediction is equal to the
vertical distance between the highest density and the second-highest, for a
particular value of x. For \code{"crossover distance"}, the x positions
where two densities cross is firstly calculated.  The predicted class is the
class with the highest density at the particular value of x and the weight
is the distance of x from the nearest density crossover point.}

\item{minDifference}{Default: 0. The minimum difference in density height
between the highest density and second-highest for a feature to be allowed
to vote. If no features for a particular sample have a difference large enough,
the class predicted is simply the largest class.}

\item{returnType}{Default: \code{"both"}. Either \code{"class"},
\code{"score"} or \code{"both"}.  Sets the return value from the prediction
to either a vector of predicted classes, a matrix of scores with columns
corresponding to classes, as determined by the factor levels of
\code{classes}, or both a column of predicted classes and columns of class
scores in a \code{data.frame}.}

\item{verbose}{Default: 3. A number between 0 and 3 for the amount of
progress messages to give.  This function only prints progress messages if
the value is 3.}

\item{targets}{If \code{measurementsTrain} is a \code{MultiAssayExperiment}, the
names of the data tables to be used. \code{"sampleInfo"} is also a valid value
and specifies that numeric variables from the sample information data table will be
used.}
}
\value{
A vector or data frame of class prediction information (i.e. classes
and/or scores), as long as the number of samples in the test data.
}
\description{
Kernel density estimates are fitted to the training data and a naive Bayes
classifier is used to classify samples in the test data.
}
\details{
If \code{difference} is \code{"weighted"}, then a sample's predicted class
is the class with the largest sum of weights, each scaled for the number of
samples in the training data of each class. Otherwise, when
\code{difference} is \code{"unweighted"}, each feature has an equal vote,
and votes for the class with the largest weight, scaled for class sizes in
the training set.

The variable name of each feature's measurements in the iteration over all
features is \code{featureValues}.  This is important to know if each
feature's measurements need to be referred to in the specification of
\code{densityParameters}, such as for specifying the range of x values of
the density function to be computed.  For example, see the default value of
\code{densityParameters} above.

If \code{weight} is \code{"crossover distance"}, the crossover points are
computed by considering the distance between y values of all of the
densities at every x value. x values for which a class density crosses any
other class' density are used as the crossover points for that class.
}
\examples{

  trainMatrix <- matrix(rnorm(1000, 8, 2), nrow = 10)
  classesTrain <- factor(rep(c("Poor", "Good"), each = 5))
  rownames(trainMatrix) <- paste("Sample", 1:10)
  
  # Make first 30 genes increased in value for poor samples.
  trainMatrix[1:5, 1:30] <- trainMatrix[1:5, 1:30] + 5
  
  testMatrix <- matrix(rnorm(1000, 8, 2), nrow = 10)
  rownames(testMatrix) <- paste("Sample", 11:20)
  
  # Make first 30 genes increased in value for sixth to tenth samples.
  testMatrix[6:10, 1:30] <- testMatrix[6:10, 1:30] + 5
  
  naiveBayesKernel(trainMatrix, classesTrain, testMatrix)

}
\author{
Dario Strbenac, John Ormerod
}
