% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ROCplot.R, R/rankingPlot.R
\name{ROCplot}
\alias{ROCplot}
\alias{ROCplot,list-method}
\alias{rankingPlot}
\alias{rankingPlot,list-method}
\title{Plot Receiver Operating Curve Graphs for Classification Results}
\usage{
ROCplot(results, ...)

\S4method{ROCplot}{list}(
  results,
  mode = c("merge", "average"),
  interval = 95,
  comparison = "Classifier Name",
  lineColours = NULL,
  lineWidth = 1,
  fontSizes = c(24, 16, 12, 12, 12),
  labelPositions = seq(0, 1, 0.2),
  plotTitle = "ROC",
  legendTitle = NULL,
  xLabel = "False Positive Rate",
  yLabel = "True Positive Rate",
  plot = TRUE,
  showAUC = TRUE
)

rankingPlot(results, ...)

\S4method{rankingPlot}{list}(
  results,
  topRanked = seq(10, 100, 10),
  comparison = "within",
  referenceLevel = NULL,
  characteristicsList = list(),
  orderingList = list(),
  sizesList = list(lineWidth = 1, pointSize = 2, legendLinesPointsSize = 1, fonts =
    c(24, 16, 12, 12, 12, 16)),
  lineColours = NULL,
  xLabelPositions = seq(10, 100, 10),
  yMax = 100,
  title = if (comparison[1] == "within") "Feature Ranking Stability" else
    "Feature Ranking Commonality",
  yLabel = if (is.null(referenceLevel)) "Average Common Features (\%)" else
    paste("Average Common Features with", referenceLevel, "(\%)"),
  margin = grid::unit(c(1, 1, 1, 1), "lines"),
  showLegend = TRUE,
  plot = TRUE,
  parallelParams = bpparam()
)
}
\arguments{
\item{results}{A list of \code{\link{ClassifyResult}} objects.}

\item{mode}{Default: "merge". Whether to merge all predictions of all
iterations of cross-validation into one set or keep them separate. Keeping
them separate will cause separate ROC curves to be computed for each
iteration and confidence intervals to be drawn with the solid line being the
averaged ROC curve.}

\item{interval}{Default: 95 (percent). The percent confidence interval to
draw around the averaged ROC curve, if mode is \code{"each"}.}

\item{comparison}{Default: within. The aspect of the experimental design to
compare. Can be any characteristic that all results share or special value
"within" to compared between all pairwise iterations of cross-validation.}

\item{lineColours}{A vector of colours for different levels of the line
colouring parameter, if one is specified by
\code{characteristicsList[["lineColour"]]}. If none are specified but,
\code{characteristicsList[["lineColour"]]} is, an automatically-generated
palette will be used.}

\item{lineWidth}{A single number controlling the thickness of lines drawn.}

\item{fontSizes}{A vector of length 5. The first number is the size of the
title.  The second number is the size of the axes titles and AUC text, if it
is not part of the legend. The third number is the size of the axes values.
The fourth number is the size of the legends' titles. The fifth number is
the font size of the legend labels.}

\item{labelPositions}{Default: 0.0, 0.2, 0.4, 0.6, 0.8, 1.0. Locations where
to put labels on the x and y axes.}

\item{plotTitle}{An overall title for the plot.}

\item{legendTitle}{A default name is used if the value is \code{NULL}.
Otherwise a character name can be provided.}

\item{xLabel}{Label to be used for the x-axis of false positive rate.}

\item{yLabel}{Label to be used for the y-axis of overlap percentages.}

\item{plot}{Logical. If \code{TRUE}, a plot is produced on the current
graphics device.}

\item{showAUC}{Logical. If \code{TRUE}, the AUC value of each result is
added to its legend text.}

\item{topRanked}{A sequence of thresholds of number of the best features to
use for overlapping.}

\item{referenceLevel}{The level of the comparison factor to use as the
reference to compare each non-reference level to. If \code{NULL}, then each
level has the average pairwise overlap calculated to all other levels.}

\item{characteristicsList}{A named list of characteristics. The name must be
one of \code{"lineColour"}, \code{"pointType"}, \code{"row"} or
\code{"column"}. The value of each element must be a characteristic name, as
stored in the \code{"characteristic"} column of the results' characteristics
table.}

\item{orderingList}{An optional named list. Any of the variables specified
to \code{characteristicsList} can be the name of an element of this list and
the value of the element is the order in which the factor should be
presented in.}

\item{sizesList}{Default: \code{lineWidth = 1, pointSize = 2,
legendLinesPointsSize = 1, fonts = c(24, 16, 12, 12, 12, 16)}. A list which
must contain elements named \code{lineWidth}, \code{pointSize},
\code{legendLinesPointsSize} and \code{fonts}. The first three specify the
size of lines and points in the graph, as well as in the plot legend.
\code{fonts} is a vector of length 6.  The first element is the size of the
title text. The second element is the size of the axes titles.  The third
element is the size of the axes values. The fourth element is the size of
the legends' titles.  The fifth element is the font size of the legend
labels. The sixth element is the font size of the titles of grouped plots,
if any are produced. Each list element must numeric.}

\item{xLabelPositions}{Locations where to put labels on the x-axis.}

\item{yMax}{The maximum value of the percentage to plot.}

\item{title}{An overall title for the plot.}

\item{margin}{The margin to have around the plot.}

\item{showLegend}{If \code{TRUE}, a legend is plotted next to the plot. If
FALSE, it is hidden.}

\item{parallelParams}{An object of class \code{\link{MulticoreParam}} or
\code{\link{SnowParam}}.}
}
\value{
An object of class \code{ggplot} and a plot on the current graphics
device, if \code{plot} is \code{TRUE}.

An object of class \code{ggplot} and a plot on the current graphics
device, if \code{plot} is \code{TRUE}.
}
\description{
Creates one ROC plot or multiple ROC plots for a list of ClassifyResult
objects.  One plot is created if the data set has two classes and multiple
plots are created if the data set has three or more classes.

Pair-wise overlaps can be done for two types of analyses. Firstly, each
cross-validation iteration can be considered within a single classification.
This explores the feature ranking stability. Secondly, the overlap may be
considered between different classification results. This approach compares
the feature ranking commonality between different results. Two types of
commonality are possible to analyse. One summary is the average pair-wise
overlap between all possible pairs of results. The second kind of summary is
the pair-wise overlap of each level of the comparison factor that is not the
reference level against the reference level. The overlaps are converted to
percentages and plotted as lineplots.
}
\details{
The scores stored in the results should be higher if the sample is more
likely to be from the class which the score is associated with. The score
for each class must be in a column which has a column name equal to the
class name.

For cross-validated classification, all predictions from all iterations are
considered simultaneously, to calculate one curve per classification.

If \code{comparison} is \code{"within"}, then the feature selection overlaps
are compared within a particular analysis. The result will inform how stable
the selections are between different iterations of cross-validation for a
particular analysis. Otherwise, the comparison is between different
cross-validation runs, and this gives an indication about how common are the
features being selected by different classifications.

Calculating all pair-wise set overlaps for a large cross-validation result
can be time-consuming.  This stage can be done on multiple CPUs by providing
the relevant options to \code{parallelParams}.
}
\examples{

  predicted <- do.call(rbind, list(data.frame(data.frame(sample = LETTERS[c(1, 8, 15, 3, 11, 20, 19, 18)],
                               Healthy = c(0.89, 0.68, 0.53, 0.76, 0.13, 0.20, 0.60, 0.25),
                               Cancer = c(0.11, 0.32, 0.47, 0.24, 0.87, 0.80, 0.40, 0.75),
                               fold = 1)),
                    data.frame(sample = LETTERS[c(11, 18, 15, 4, 6, 10, 11, 12)],
                               Healthy = c(0.45, 0.56, 0.33, 0.56, 0.33, 0.20, 0.60, 0.40),
                               Cancer = c(0.55, 0.44, 0.67, 0.44, 0.67, 0.80, 0.40, 0.60),
                               fold = 2)))
  actual <- factor(c(rep("Healthy", 10), rep("Cancer", 10)), levels = c("Healthy", "Cancer"))
  result1 <- ClassifyResult(DataFrame(characteristic = c("Data Set", "Selection Name", "Classifier Name",
                                                         "Cross-validation"),
                            value = c("Melanoma", "t-test", "Random Forest", "2 Permutations, 2 Folds")),
                            LETTERS[1:20], LETTERS[10:1],
                            list(1:100, c(1:9, 11:101)), list(sample(10, 10), sample(10, 10)),
                            list(function(oracle){}), NULL, predicted, actual)
  
  predicted[c(2, 6), "Healthy"] <- c(0.40, 0.60)
  predicted[c(2, 6), "Cancer"] <- c(0.60, 0.40)
  result2 <- ClassifyResult(DataFrame(characteristic = c("Data Set", "Selection Name", "Classifier Name",
                                                         "Cross-validation"),
                            value = c("Example", "Bartlett Test", "Differential Variability", "2 Permutations, 2 Folds")),
                            LETTERS[1:20], LETTERS[10:1], list(1:100, c(1:5, 11:105)),
                            list(sample(10, 10), sample(10, 10)), list(function(oracle){}),
                            NULL, predicted, actual)
  ROCplot(list(result1, result2), plotTitle = "Cancer ROC")


  predicted <- data.frame(sample = sample(10, 100, replace = TRUE),
                          permutation = rep(1:2, each = 50),
                          class = rep(c("Healthy", "Cancer"), each = 50))
  actual <- factor(rep(c("Healthy", "Cancer"), each = 5))
  allFeatures <- sapply(1:100, function(index) paste(sample(LETTERS, 3), collapse = ''))
  rankList <- list(allFeatures[1:100], allFeatures[c(15:6, 1:5, 16:100)],
                   allFeatures[c(1:9, 11, 10, 12:100)], allFeatures[c(1:50, 61:100, 60:51)])
  result1 <- ClassifyResult(DataFrame(characteristic = c("Data Set", "Selection Name", "Classifier Name",
                                                         "Cross-validation"),
                            value = c("Melanoma", "t-test", "Diagonal LDA", "2 Permutations, 2 Folds")),
                            LETTERS[1:10], allFeatures, rankList,
                            list(rankList[[1]][1:15], rankList[[2]][1:15],
                                 rankList[[3]][1:10], rankList[[4]][1:10]),
                            list(function(oracle){}), NULL,
                            predicted, actual)
  
  predicted[, "class"] <- sample(predicted[, "class"])
  rankList <- list(allFeatures[1:100], allFeatures[c(sample(20), 21:100)],
                   allFeatures[c(1:9, 11, 10, 12:100)], allFeatures[c(1:50, 60:51, 61:100)])
  result2 <- ClassifyResult(DataFrame(characteristic = c("Data Set", "Selection Name", "Classifier Name",
                                                         "Cross-validations"),
                            value = c("Melanoma", "t-test", "Random Forest", "2 Permutations, 2 Folds")),
                            LETTERS[1:10], allFeatures, rankList,
                            list(rankList[[1]][1:15], rankList[[2]][1:15],
                                 rankList[[3]][1:10], rankList[[4]][1:10]),
                            list(function(oracle){}), NULL,
                            predicted, actual)
                            
  rankingPlot(list(result1, result2), characteristicsList = list(pointType = "Classifier Name"))

}
\author{
Dario Strbenac

Dario Strbenac
}
