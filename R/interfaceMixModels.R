#' Classification based on Differential Distribution utilising Mixtures of
#' Normals
#' 
#' Fits mixtures of normals for every feature, separately for each class.
#' 
#' If \code{weighted} is \code{TRUE}, then a sample's predicted class is the
#' class with the largest sum of weights, each scaled for the number of samples
#' in the training data of each class. Otherwise, when \code{weighted} is
#' \code{FALSE}, each feature has an equal vote, and votes for the class with
#' the largest weight, scaled for class sizes in the training set.
#' 
#' If \code{weight} is \code{"crossover distance"}, the crossover points are
#' computed by considering the distance between y values of the two densities
#' at every x value. x values for which the sign of the difference changes
#' compared to the difference of the closest lower value of x are used as the
#' crossover points.
#' 
#' @aliases mixmodels mixModelsTrain mixModelsTrain,matrix-method
#' mixModelsTrain,DataFrame-method mixModelsTrain,MultiAssayExperiment-method
#' mixModelsPredict mixModelsPredict,MixModelsListsSet,matrix-method
#' mixModelsPredict,MixModelsListsSet,DataFrame-method
#' mixModelsPredict,MixModelsListsSet,MultiAssayExperiment-method
#' @param measurementsTrain Either a \code{\link{matrix}}, \code{\link{DataFrame}}
#' or \code{\link{MultiAssayExperiment}} containing the training data.  For a
#' \code{matrix} or \code{\link{DataFrame}}, the rows are samples, and the columns are features.
#' If of type \code{\link{DataFrame}} or \code{\link{MultiAssayExperiment}}, the data set is subset
#' to only those features of type \code{numeric}.
#' @param classesTrain A vector of class labels of class \code{\link{factor}} of the
#' same length as the number of samples in \code{measurementsTrain} if it is a
#' \code{\link{matrix}} or a \code{\link{DataFrame}} or a character vector of length 1
#' containing the column name in \code{measurementsTrain} if it is a \code{\link{DataFrame}} or the
#' column name in \code{colData(measurementsTrain)} if \code{measurementsTrain} is a
#' \code{\link{MultiAssayExperiment}}. If a column name, that column will be
#' removed before training.
#' @param measurementsTest An object of the same class as \code{measurementsTrain} with no
#' samples in common with \code{measurementsTrain} and the same number of features
#' as it.
#' @param targets If \code{measurements} is a \code{MultiAssayExperiment}, the
#' names of the data tables to be used. \code{"sampleInfo"} is also a valid value
#' and specifies that numeric variables from the clinical data table will be
#' used.
#' @param ... Variables not used by the \code{matrix} nor the
#' \code{MultiAssayExperiment} method which are passed into and used by the
#' \code{DataFrame} method or extra arguments for training passed to
#' \code{\link[Rmixmod]{mixmodCluster}}. The argument \code{nbCluster} is
#' mandatory.
#' @param models A \code{MixModelsListsSet} of models generated by the
#' training function and training class information. There is one element for
#' each class. Another element at the end of the list has the class sizes of
#' the classes in the training data.
#' @param difference Default: \code{"unweighted"}. Either \code{"unweighted"}
#' or \code{"weighted"}. In weighted mode, the difference in densities is
#' summed over all features. If unweighted mode, each feature's vote is worth
#' the same. Both can be calculated simultaneously.
#' @param weighting Default: \code{"height difference"}. Either \code{"height
#' difference"}, or \code{"crossover distance"}. The type of weight to
#' calculate. For \code{"height difference"}, the weight of each prediction is
#' equal to the sum of the vertical distances for all of the mixture components
#' within one class subtracted from the sum of the components of the other
#' class, summed for each value of x. For \code{"crossover distance"}, the x
#' positions where the mixture density of the class being considered crosses
#' another class' density is firstly calculated. The predicted class is the
#' class with the highest mixture sum at the particular value of x and the
#' weight is the distance of x from the nearest density crossover point.
#' @param densityXvalues Default: 1024. Only relevant when \code{weight} is
#' \code{"crossover distance"}.  The number of equally-spaced locations at
#' which to calculate y values for each mixture density.
#' @param minDifference Default: 0. The minimum difference in sums of mixture
#' densities between the class with the highest sum and the class with the
#' second highest sum for a feature to be allowed to vote. If no features for
#' a particular sample have a difference large enough, the class predicted is
#' simply the largest class.
#' @param returnType Default: \code{"both"}. Either \code{"class"},
#' \code{"score"} or \code{"both"}.  Sets the return value from the prediction
#' to either a vector of predicted classes, a matrix of scores with columns
#' corresponding to classes, as determined by the factor levels of
#' \code{classesTrain}, or both a column of predicted classes and columns of class
#' scores in a \code{data.frame}.
#' @param verbose Default: 3. A number between 0 and 3 for the amount of
#' progress messages to give.  This function only prints progress messages if
#' the value is 3.
#' @return For \code{mixModelsTrain}, a list of trained models of class
#' \code{\link[Rmixmod:MixmodCluster-class]{MixmodCluster}}.  For
#' \code{mixModelsPredict}, a vector of class prediction information
#' (i.e. classes and/or scores), as long as the number of samples in the test
#' data.
#' @author Dario Strbenac
#' @examples
#' 
#'   # First 25 samples and first 5 genes are mixtures of two normals. Last 25 samples are
#'   # one normal.
#'   
#'   genesMatrix <- t(sapply(1:25, function(geneColumn) c(rnorm(5, sample(c(5, 15), replace = TRUE, 5)))))
#'   genesMatrix <- rbind(genesMatrix, sapply(1:5, function(geneColumn) c(rnorm(25, 9, 1))))
#'   genesMatrix <- cbind(genesMatrix, sapply(1:5, function(geneColumn) rnorm(50, 9, 1)))
#'   rownames(genesMatrix) <- paste("Sample", 1:50)
#'   colnames(genesMatrix) <- paste("Gene", 1:10)
#'   classes <- factor(rep(c("Poor", "Good"), each = 25), levels = c("Good", "Poor"))
#'   
#'   trainSamples <- c(1:15, 26:40)
#'   testSamples <- c(16:25, 41:50)
#'   selected <- 1:5
#'   
#'   trained <- mixModelsTrain(genesMatrix[trainSamples, selected], classes[trainSamples],
#'                             nbCluster = 1:3)
#'   mixModelsPredict(trained, genesMatrix[testSamples, selected])
#' 
#' @rdname mixModels
#' @usage NULL
#' @export
setGeneric("mixModelsTrain", function(measurementsTrain, ...) standardGeneric("mixModelsTrain"))

#' @rdname mixModels
#' @export
setMethod("mixModelsTrain", "matrix", function(measurementsTrain, ...) # Matrix of numeric measurements.
{
  mixModelsTrain(DataFrame(measurementsTrain, check.names = FALSE), ...)
})

#' @rdname mixModels
#' @export
setMethod("mixModelsTrain", "DataFrame", function(measurementsTrain, classesTrain, ..., verbose = 3) # Mixed data types.
{
  splitDataset <- .splitDataAndOutcomes(measurementsTrain, classesTrain)
  measurementsTrain <- splitDataset[["measurements"]]
  classesTrain <- splitDataset[["outcomes"]]

  if(verbose == 3)
    message("Fitting mixtures of normals for features.")
  if(!requireNamespace("Rmixmod", quietly = TRUE))
    stop("The package 'Rmixmod' could not be found. Please install it.")

  models <- lapply(levels(classesTrain), function(class)
            {
                aClassMeasurements <- measurementsTrain[classesTrain == class, , drop = FALSE]
                apply(aClassMeasurements, 2, function(featureColumn)
                {
                   mixmodParams <- list(featureColumn)
                   mixmodParams <- append(mixmodParams, list(...))
                   do.call(Rmixmod::mixmodCluster, mixmodParams)
                })
            })

  if(verbose == 3)
    message("Done fitting normal mixtures.")

  models <- lapply(models, function(modelSet)
            {
              class(modelSet) <- "MixModelsList"
              modelSet
            })
  names(models)[1:length(levels(classesTrain))] <- paste(levels(classesTrain), "Models", sep = '')
  models[["classSizes"]] <- setNames(as.vector(table(classesTrain)), levels(classesTrain))
  models <- MixModelsListsSet(models)
  models
})

# One or more omics data sets, possibly with sample information data.
#' @rdname mixModels
#' @export
setMethod("mixModelsTrain", "MultiAssayExperiment", function(measurementsTrain, targets = names(measurementsTrain), classesTrain, ...)
{
  tablesAndClasses <- .MAEtoWideTable(measurementsTrain, targets, classesTrain)
  dataTable <- tablesAndClasses[["dataTable"]]
  classesTrain <- tablesAndClasses[["outcomes"]]
  mixModelsTrain(dataTable, classesTrain, ...)
})

#' @rdname mixModels
#' @usage NULL
#' @export
setGeneric("mixModelsPredict", function(models, measurementsTest, ...) standardGeneric("mixModelsPredict"))

#' @rdname mixModels
#' @export
setMethod("mixModelsPredict", c("MixModelsListsSet", "matrix"), function(models, measurementsTest, ...)
{
  mixModelsPredict(models, DataFrame(measurementsTest, check.names = FALSE), ...)
})

#' @rdname mixModels
#' @export
setMethod("mixModelsPredict", c("MixModelsListsSet", "DataFrame"), # Sample information data, perhaps.
          function(models, measurementsTest, difference = c("unweighted", "weighted"),
                   weighting = c("height difference", "crossover distance"),
                   densityXvalues = 1024, minDifference = 0,
                   returnType = c("both", "class", "score"), verbose = 3)
{
  models <- models@set
  isNumeric <- sapply(measurementsTest, is.numeric)
  measurementsTest <- measurementsTest[, isNumeric, drop = FALSE]
  if(sum(isNumeric) == 0)
    stop("No features are numeric but at least one must be.")

  difference <- match.arg(difference)
  weighting <- match.arg(weighting)
  returnType <- match.arg(returnType)
  classesNames <- names(models[["classSizes"]])
  classesSizes <- models[["classSizes"]]
  largestClass <- names(classesSizes)[which.max(classesSizes)[1]]
  models <- models[-length(models)]

  if(verbose == 3)
    message("Predicting using normal mixtures.")

  featuresDensities <- lapply(1:ncol(measurementsTest), function(featureIndex)
  {
    featureValues <- unlist(lapply(models, function(classModels) classModels[[featureIndex]]@data))
    xValues <- seq(min(featureValues), max(featureValues), length.out = densityXvalues)
    setNames(lapply(models, function(model)
    {
      yValues <- Reduce('+', lapply(1:model[[featureIndex]]@bestResult@nbCluster, function(index)
      {
        model[[featureIndex]]@bestResult@parameters@proportions[index] * dnorm(xValues, model[[featureIndex]]@bestResult@parameters@mean[index], sqrt(as.numeric(model[[featureIndex]]@bestResult@parameters@variance[[index]])))
      }))
      list(x = xValues, y = yValues)
    }), classesNames)
  })

  splines <- lapply(featuresDensities, function(featureDensities)
             {
               lapply(featureDensities, function(classDensities)
               {
                 splinefun(classDensities[['x']], classDensities[['y']], "natural")
               })
             })

  if(verbose == 3)
    message("Calculating vertical differences between normal mixture densities.")

  # Needed even if horizontal distance weighting is used to determine the predicted class.
  posteriorsVertical <- mapply(function(featureSplines, testSamples)
  {
    sapply(1:length(classesNames), function(classIndex)
    {
      featureSplines[[classIndex]](testSamples)
    })
  }, splines, measurementsTest, SIMPLIFY = FALSE)

  classesVertical <- sapply(posteriorsVertical, function(featureVertical)
  {
      apply(featureVertical, 1, function(sampleVertical) classesNames[which.max(sampleVertical)])
  }) # Matrix, rows are test samples, columns are features.

  distancesVertical <- sapply(posteriorsVertical, function(featureVertical)
  { # Vertical distance between highest density and second-highest, at a particular value.
    apply(featureVertical, 1, function(sampleVertical)
    {
      twoHighest <- sort(sampleVertical, decreasing = TRUE)[1:2]
      Reduce('-', twoHighest)
    })
  }) # Matrix, rows are test samples, columns are features.

  if(difference == "crossover distance")
  {
    if(verbose == 3)
      message("Calculating horizontal distances to crossover points of class densities.")

    classesVerticalIndices <- matrix(match(classesVertical, classesNames),
                                     nrow = nrow(classesVertical), ncol = ncol(classesVertical))
    distancesHorizontal <- mapply(function(featureDensities, testSamples, predictedClasses)
    {
      classesCrosses <- .densitiesCrossover(featureDensities)
      classesDistances <- sapply(classesCrosses, function(classCrosses)
      {
        sapply(testSamples, function(testSample) min(abs(testSample - classCrosses)))
      })
      classesDistances[cbind(1:nrow(classesDistances), predictedClasses)]
    }, featuresDensities, measurementsTest, as.data.frame(classesVerticalIndices)) # Matrix of horizontal distances to nearest cross-over involving the predicted class.
  }

  if(verbose == 3)
  {
    switch(returnType, class = message("Determining class labels."),
                       both = message("Calculating class scores and determining class labels."),
                       score = message("Calculating class scores.")
    )
  }

  allDistances <- switch(weighting, `height difference` = distancesVertical,
                         `crossover distance` = distancesHorizontal)

  predictions <- do.call(rbind, lapply(1:nrow(allDistances), function(sampleRow)
  {
    useFeatures <- abs(allDistances[sampleRow, ]) > minDifference
    if(all(useFeatures == FALSE)) # No features have a large enough density difference.
    {                          # Simply vote for the larger class.
      classPredicted <- largestClass
      classScores <- classesSizes / sum(classesSizes)
    } else { # One or more features are available to vote with.
      distancesUsed <- allDistances[sampleRow, useFeatures]
      classPredictionsUsed <- factor(classesVertical[sampleRow, useFeatures], classesNames)
      if(difference == "unweighted")
      {
        classScores <- table(classPredictionsUsed)
        classScores <- setNames(as.vector(classScores), classesNames)
      } else { # Weighted voting.
        classScores <- tapply(distancesUsed, classPredictionsUsed, sum)
        classScores[is.na(classScores)] <- 0
      }
      classScores <- classScores / sum(classScores) # Make different feature selection sizes comparable.
      classPredicted <- names(classScores)[which.max(classScores)]
    }

    data.frame(class = factor(classPredicted, levels = classesNames), t(classScores), check.names = FALSE)
  }))

  switch(returnType, class = predictions[, "class"],
         score = predictions[, colnames(predictions) %in% classesNames],
         both = data.frame(class = predictions[, "class"], predictions[, colnames(predictions) %in% classesNames, drop = FALSE], check.names = FALSE)
  )
})

#' @rdname mixModels
#' @export
# One or more omics data sets, possibly with sample information data.
setMethod("mixModelsPredict", c("MixModelsListsSet", "MultiAssayExperiment"), function(models, measurementsTest, targets = names(measurementsTest), ...)
{
  testingMatrix <- .MAEtoWideTable(measurementsTest, targets)
  mixModelsPredict(models, testingMatrix, ...)
})