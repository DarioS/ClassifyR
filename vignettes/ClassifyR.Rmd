---
title: "An Introduction to **ClassifyR**"
author: Dario Strbenac, Graham Mann, Jean Yang, John Ormerod <br>
        The University of Sydney, Australia.
output: 
  BiocStyle::html_document:
    toc: true
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{An Introduction to the ClassifyR Package}
---

<style>
    body .main-container {
        max-width: 1600px;
    }
    p {
      padding: 20px;
    }
</style>

```{r, echo = FALSE, results = "asis"}
options(width = 130)
```

## Overview

**ClassifyR** provides a structured pipeline for cross-validated classification. Classification is viewed in terms of four stages, data transformation, feature selection, classifier training, and prediction. The stages can be run in any order that is sensible.

Each step can be provided with custom functions that follow some rules about parameters. The driver function *runTests* implements different varieties of cross-validation. They are:

* Ordinary k-fold cross-validation
* Permutation of the order of samples followed by k-fold cross-validation
* Repeated x% test set cross-validation
* leave-k-out cross-validation

*runTests* can use parallel processing capabilities in R to speed up cross-validations when many CPUs are available. The output of *runTests* is a *ClassifyResult* object which can be directly used by the performance evaluation functions. The process of classification is summarised by a flowchart.


```{r, echo = FALSE}
htmltools::img(src = knitr::image_uri("ClassifyRprocedure.png"), 
               style = 'margin-left: auto;margin-right: auto')
```

Importantly, ClassifyR implements a number of methods for classification using different kinds of changes in measurements between classes. Most classifiers work with features where the means are different. In addition to changes in means (DM), **ClassifyR** also allows for classification using differential variability (DV; changes in scale) and differential distribution (DD; changes in location and/or scale).

Load the package.

```{r}
library(ClassifyR)
```

## Quick Start

If the intention is to quickly apply a few different algorithms and compare their results without going into too much of the details, a convenience function is provided for the most common cross-validation setting and classifiers by the function *quickClassify*. For example, the code below will carry out a standard cross-validation for differences in means using 100 permutations and five folds.

```{r, eval = FALSE}
result <- quickClassify(measurements, classes, "100 permutations, 5 folds", "DLDA", cores = 16)
```

Similarly, leave-one-out cross-validation with the naive Bayes classifier for a differential distriution style of classification could be done by the example below.

```{r, eval = FALSE}
result <- quickClassify(measurements, classes, "Leave-one-out", "naive Bayes", cores = 16)
```

For a full list of shortcuts, please view *?quickClassify*.

## A Closer Look

In the following sections, some of the most useful functions provided in **ClassifyR** will be demonstrated. However, a user can provide any feature selection, training, or prediction function to the classification framework, as long as it meets some simple rules about the input and return parameters. See the appendix section of this guide titled "Rules for New Functions" for a description of these.

### Comparison to Existing Classification Frameworks

There are a few other frameworks for classification in R. The table below provides a comparison of which features they offer.

Package | Run User-defined Classifiers | Parallel Execution on any OS | Parameter Tuning | Intel DAAL Performance Metrics | Ranking and Selection Plots | Class Distribution Plot | Sample-wise Error Heatmap | Direct Support for MultiAssayExperiment Input
-------|--------|-------|--------|--------|---------|-----------|----------|----------
**ClassifyR**    | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes
caret        | Yes | Yes | Yes | No  | No  | No  | No  | No
MLInterfaces | Yes | No  | No  | No  | No  | No  | No  | No
MCRestimate  | Yes | No  | Yes | No  | No  | No  | No  | No
CMA          | No  | No  | Yes | No  | No  | No  | No  | No

### Provided Functionality

Although being a cross-validation framework, a number of popular feature selection and classification functions are provided by the package which meet the requirements of functions to be used by it (see the last section).

#### Provided Methods for Feature Selection and Classification

Functions with names ending in "interface" indicate wrappers for existing methods implemented in other packages. Different methods select different types of changes (i.e. location and/or scale) between classes.

```{r, echo = FALSE}
htmltools::img(src = knitr::image_uri("providedSelection.png"), 
               style = 'margin-left: auto;margin-right: auto')
```

Likewise, a variety of classifiers is also provided.

```{r, echo = FALSE}
htmltools::img(src = knitr::image_uri("providedClassifiers.png"), 
               style = 'margin-left: auto;margin-right: auto')
```

If a desired selection or classification method is not already implemented, rules for writing functions to work with **ClassifyR** are outlined in the wrapper vignette. Please visit it for more information.

#### Provided Meta-feature Methods

A number of methods are provided for users to enable classification in a feature-set-centric way. The meta-feature creation functions should be used before cross-validation is done.

```{r, echo = FALSE}
htmltools::img(src = knitr::image_uri("networkFunctions.png"), 
               style = 'margin-left: auto;margin-right: auto')
```

### Cross-validation and Modelling

For cross-validation, all of the parameters are specified by a CrossValParams object. The default setting are for 100 permutations and five folds and parameter tuning is done by resubstitution. It is also recommended to specify a *parallelParams* setting. On Linux and MacOS operating systems, it should be *MulticoreParam* and on Windows computers it should be *SnowParam*. Note that each of these have an option *RNGseed* and this **needs to be set by the user** because some classifiers or feature selection functions will have some element of randomisation. One example that work on all operating systems, but intended for Windows is:

```{r, eval = FALSE}
CVparams <- CrossValParams(parallelParams = SnowParam(16, RNGseed = 123))
CVparams
```

For the actual operations to do to the data to build a model of it, each of the stages should be specified by an object of class *ModellingParams*. This controls how class imbalance is handled (default is to downsample to the smallest class), any transformation that needs to be done inside of cross-validation, any feature selection and the training and prediction functions to be used. The default is to do an ordinary t-test (two groups) or ANOVA (three or more group) and classification using diagonal LDA.

```{r}
ModellingParams()
```

### Pre-validation

Pre-validation is an approach to provide a fairer way to compare the benefit of omics data to traditional, freely-available clinical data. For each omics technology considered, training and testing is done on all of the various partitions of the data and the predictions of each sample are simply added as a column to the clinical data table. Then, cross-validation is done as usual. If an omics data set is important, it will often be included as a selected feature.

Pre-validation is activated by specifying a list of *ModellingParams* objects with one of them named *"prevaliated"*, which specifies the kind of classification to do on the resultant augmented clinical data table. The classification procedure is typically a logistic regression type, such as Elastic net regularised regression. Other lists must be named with each name matching an assay in the measurements object, which must be of type [*MultiAssayExperiment*](https://bioconductor.org/packages/release/bioc/html/MultiAssayExperiment.html). For example, if a data object had two assays named RNA and protein, as well as some clinical data about the patients, then a suitable specification of *modellingParams* for the function *runTests* would be:

```{r, message = FALSE, eval = FALSE}
# Coming soon.
```

*elasticNetFeatures* is a function that enables selected features to be extracted from the trained models, which are simply the variables with a beta coefficient that is not zero. Unlike most classifiers, for elastic net GLM, the feature selection happens during model training - not independently before it.

### Case Study : Diagnosing Asthma

To demonstrate some key features of ClassifyR, a data set consisting of the 2000 most variably expressed genes and 190 people will be used to quickly obtain results. The journal article corresponding to the data set was published in *Scientific Reports* in 2018 and is titled [A Nasal Brush-based Classifier of Asthma Identified by Machine Learning Analysis of Nasal RNA Sequence Data](http://www.nature.com/articles/s41598-018-27189-4).

```{r, message = FALSE}
data(asthma) # Contains measurements and classes variables.
measurements[1:5, 1:5]
head(classes)
```

The numeric matrix variable *measurements* stores the normalised values of the RNA gene abundances for each sample and the factor vector *classes* identifies which class the samples belong to. The measurements were normalised using **DESeq2**'s *varianceStabilizingTransformation* function, which produces $log_2$-like data.

For more complex data sets with multiple kinds of experiments (e.g. DNA methylation, copy number, gene expression on the same set of samples) a [*MultiAssayExperiment*](https://bioconductor.org/packages/release/bioc/html/MultiAssayExperiment.html) is recommended for data storage and supported by **ClassifyR**'s methods.

### runTests Driver Function of Cross-validated Classification

*runTests* is the main function in **ClassifyR** which handles the sample splitting and parallelisation, if used, of cross-validation. To begin with, a simple classifier will be demonstrated. It uses a t-test or ANOVA ranking (depending on the number of classes) for feature ranking and DLDA for classification. This classifier relies on differences in means between classes. No parameters need to be specified, because this is the default classification of *runTests*. By default, the number of features is tuned by resubstitution on the training set.

```{r, tidy = FALSE}
crossValParams <- CrossValParams(permutations = 20)
DMresults <- runTests(measurements, classes, crossValParams, verbose = 1)
DMresults
```

Here, 20 permutations (non-default) and 5 folds cross-validation (default) is specified. For computers with more than 1 CPU, the number of cores to use can be given to *runTests* by using the argument *parallelParams*. The parameter *seed* is important to set for result reproducibility when doing a cross-validation such as this, because it employs randomisation to partition the samples into folds. Also, *RNGseed* is highly recommended to be set to the back-end specified to *BPPARAM* if doing paralle processing. The first seed mentioned does not work for parallel processes. For more details about *runTests* and the parameter classes used by it, consult the help pages of such functions.

### Evalulation of a Classification

The most frequently selected gene can be identified using the *distribution* function and its relative abundance values for all samples can be displayed visually by *plotFeatureClasses*.

```{r, fig.height = 8, fig.width = 8, results = "hold", message = FALSE}
selectionPercentages <- distribution(DMresults, plot = FALSE)
sortedPercentages <- sort(selectionPercentages, decreasing = TRUE)
head(sortedPercentages)
mostChosen <- names(sortedPercentages)[1]
bestGenePlot <- plotFeatureClasses(measurements, classes, mostChosen, dotBinWidth = 0.1,
                                   xAxisLabel = "Normalised Expression")
```

The means of the abundance levels of `r names(sortedPercentages)[1]` are substantially different between the people with and without asthma. *plotFeatureClasses* can also plot categorical data, such as may be found in a clinical data table, as a bar chart.

Classification error rates, as well as many other prediction performance measures, can be calculated with *calcCVperformance*. Next, the balanced error rate is calculated considering all samples, each of which was in the test set once. The balanced error rate is defined as the average of the classification errors of each class.

See the documentation of *calcCVperformance* for a list of performance metrics which may be calculated.

```{r}
DMresults <- calcCVperformance(DMresults, "Balanced Error")
DMresults
performance(DMresults)
```

The error rate is about 20%. If only a vector of predictions and a vector of actual classes is available, such as from an old study which did not use **ClassifyR** for cross-validation, then *calcExternalPerformance* can be used on a pair of factor vectors which have the same length.

### Comparison of Different Classifications

The *samplesMetricMap* function allows the visual comparison of sample-wise error rate or accuracy measures from different *ClassifyResult* objects. Firstly, a classifier will be run that uses Kullback-Leibler divergence ranking and resubstitution error as a feature selection heuristic and a naive Bayes classifier for classification. This classification will use features that have either a change in location or in scale between classes.

```{r, tidy = FALSE}
modellingParamsDD <- ModellingParams(selectParams = SelectParams(KullbackLeiblerRanking),
                                     trainParams = TrainParams(naiveBayesKernel),
                                     predictParams = NULL)
DDresults <- runTests(measurements, classes, crossValParams, modellingParamsDD, verbose = 1)
DDresults
```

The naive Bayes kernel classifier by default uses the vertical distance between class densities but it can instead use the horizontal distance to the nearest non-zero density cross-over point to confidently classify samples in the tails of the densities.

Now, the classification error for each sample is also calculated for both the differential means and differential distribution classifiers and both *ClassifyResult* objects generated so far are plotted with *samplesMetricMap*.

```{r, fig.width = 10, fig.height = 7}
library(grid)
DMresults <- calcCVperformance(DMresults, "Sample Error")
DDresults <- calcCVperformance(DDresults, "Sample Error")
resultsList <- list(Abundance = DMresults, Distribution = DDresults)
errorPlot <- samplesMetricMap(resultsList, metric = "Sample Error", xAxisLabel = "Sample",
                              showXtickLabels = FALSE, plot = FALSE)
grid.draw(errorPlot)
```

The benefit of this plot is that it allows the easy identification of samples which are hard to classify and could be explained by considering additional information about them. Differential distribution class prediction appears to be biased to the majority class (No Asthma).

The features being ranked and selected in the feature selection stage can be compared within and between classifiers by the plotting functions *rankingPlot* and *selectionPlot*. Consider the task of visually representing how consistent the feature rankings of the top 50 different features were for the differential distribution classifier for all 5 folds in the 20 cross-validations.

```{r}
rankOverlaps <- rankingPlot(list(DDresults), topRanked = 1:100,
                            xLabelPositions = c(1, seq(10, 100, 10)),
                            plot = FALSE)
rankOverlaps
```

The top-ranked features are fairly similar between all pairs of the 20 cross-validations.

For a large cross-validation scheme, such as leave-2-out cross-validation, or when *results* contains many classifications, there are many feature set comparisons to make. Note that *rankingPlot* and *selectionPlot* have a *parallelParams* options which allows for the calculation of feature set overlaps to be done on multiple processors.

### Using an Independent Test Set

Sometimes, cross-validation is unnecessary. This happens when studies have large sample sizes and are designed such that a large number of samples is prespecified to form a test set. The classifier is only trained on the training sample set, and makes predictions only on the test sample set. This can be achieved by using the function *runTest* directly. See its documentation for required inputs.

### Cross-validating Selected Features on a Different Data Set

Once a cross-validated classification is complete, the usefulness of the features selected may be explored in another dataset. *previousSelection* is a function which takes an existing *ClassifyResult* object and returns the features selected at the equivalent iteration which is currently being processed. This is necessary, because the models trained on one data set are not directly transferrable to a new dataset; the classifier training (e.g. choosing thresholds, fitting model coefficients) is redone. Of course, the features in the new dataset should have the same naming system as the ones in the old dataset.

## Generating a ROC Plot

Some classifiers can be set to output scores or probabilities representing how likely a sample is to be from one of the classes, instead of, or as well as, class labels. This enables different score thresholds to be tried, to generate pairs of false positive and false negative rates. The naive Bayes classifier used previously has its *returnType* parameter set to *"both"* by default, so class predictions and scores are both stored in the classification result. In this case, a data frame with two columns (named "class" and "score") is returned by the classifier to the cross-validation framework. Setting *returnType* to *"score"* is also sufficient to generate a ROC plot. Many existing classifiers in other R packages also have an option that allows a score or probability to be calculated.

```{r, fig.height = 5, fig.width = 6}
ROCcurves <- ROCplot(list(DDresults), fontSizes = c(24, 12, 12, 12, 12))
```

This ROC plot shows the classifiability of the asthma data set is high. Examples of other included functions which output scores are *fisherDiscriminant*, *DLDApredictInterface*, and *SVMpredictInterface*.

## Parameter Tuning

Some feature ranking methods or classifiers allow the choosing of tuning parameters, which controls some aspect of their model learning. An example of doing parameter tuning with a linear SVM is presented. This particular SVM has a single tuning parameter, the cost. Higher values of this parameter penalise misclassifications more. Moreover, feature selection happens by using a feature ranking function and then trying a range of top-ranked features to see which gives the best performance, the range being specified by *"nFeatures"*. Therefore, some kind of parameter tuning always happens, even if the feature ranking or classifier function does not have any explicit tuning parameters.

Tuning is achieved in ClassifyR by providing a variable called *tuneParams* to the SelectParams or TrainParams constructor. *tuneParams* is a named list, with the names being the names of the tuning variables, except for one which is named *"performanceType"* and specifies the performance metric to use for picking the parameter values. Any of the non-sample-specific performance metrics which *calcCVperformance* calculates can be optimised.

```{r}
tuneList <- list(cost = c(0.01, 0.1, 1, 10))
SVMparams <- ModellingParams(trainParams = TrainParams(SVMtrainInterface, kernel = "linear", tuneParams = tuneList),
                             predictParams = PredictParams(SVMpredictInterface))
SVMresults <- runTests(measurements, classes, crossValParams, SVMparams)
```

The chosen values of the parameters are stored for every validation, and can be accessed with the *tunedParameters* function.

```{r}
length(tunedParameters(SVMresults))
tunedParameters(SVMresults)[1:5]
```

The cost value of 1 is chosen in all of the folds of the first sample permutation.

## Summary

**ClassifyR** is a framework for cross-validated classification that provides a variety of unique functions for performance evaluation. It provides wrappers for many popular classifiers but is designed to be extensible if other classifiers are desired.

## References
Strbenac D., Yang, J., Mann, G.J. and Ormerod, J. T. (2015) [ClassifyR: an R package for performance assessment of classification with applications to transcriptomics](http://bioinformatics.oxfordjournals.org/content/31/11/1851), *Bioinformatics*, 31(11):1851-1853 <br>
Strbenac D., Mann, G.J., Yang, J. and Ormerod, J. T. (2016) [Differential distribution improves gene selection stability and has competitive classification performance for patient survival](http://nar.oxfordjournals.org/content/44/13/e119), *Nucleic Acids Research*, 44(13):e119